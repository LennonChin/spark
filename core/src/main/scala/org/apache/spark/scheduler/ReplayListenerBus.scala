/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.scheduler

import java.io.{InputStream, IOException}

import scala.io.Source

import com.fasterxml.jackson.core.JsonParseException
import com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException
import org.json4s.jackson.JsonMethods._

import org.apache.spark.internal.Logging
import org.apache.spark.scheduler.ReplayListenerBus._
import org.apache.spark.util.JsonProtocol

/**
 * A SparkListenerBus that can be used to replay events from serialized event data.
  * 用于从序列化的事件数据中重播事件。
 */
private[spark] class ReplayListenerBus extends SparkListenerBus with Logging {

  /**
   * Replay each event in the order maintained in the given stream. The stream is expected to
   * contain one JSON-encoded SparkListenerEvent per line.
   *
   * This method can be called multiple times, but the listener behavior is undefined after any
   * error is thrown by this method.
   *
   * @param logData Stream containing event log data.
   * @param sourceName Filename (or other source identifier) from whence @logData is being read
   * @param maybeTruncated Indicate whether log file might be truncated (some abnormal situations
   *        encountered, log file might not finished writing) or not
   * @param eventsFilter Filter function to select JSON event strings in the log data stream that
   *        should be parsed and replayed. When not specified, all event strings in the log data
   *        are parsed and replayed.
   */
  def replay(
      logData: InputStream,
      sourceName: String,
      maybeTruncated: Boolean = false,
      eventsFilter: ReplayEventsFilter = SELECT_ALL_FILTER): Unit = {
    // 从logData指定的输入流中读取事件数据，并转换为迭代器，迭代元素都是字符串
    val lines = Source.fromInputStream(logData).getLines()
    // 使用replay()方法进行事件重播
    replay(lines, sourceName, maybeTruncated, eventsFilter)
  }

  /**
   * Overloaded variant of [[replay()]] which accepts an iterator of lines instead of an
   * [[InputStream]]. Exposed for use by custom ApplicationHistoryProvider implementations.
   *
    * @param lines 事件集合
    * @param sourceName  事件源名称
    * @param maybeTruncated
    * @param eventsFilter 事件过滤器，即(String) => Boolean函数
    */
  def replay(
      lines: Iterator[String],
      sourceName: String,
      maybeTruncated: Boolean,
      eventsFilter: ReplayEventsFilter): Unit = {
    // 记录当前行
    var currentLine: String = null
    // 记录行号
    var lineNumber: Int = 0

    try {
      // 预处理
      val lineEntries = lines
        // 对lines迭代器进行Zip操作，为每行都添加序号
        .zipWithIndex
        // 使用指定的事件过滤器进行过滤
        .filter { case (line, _) => eventsFilter(line) }

      // 遍历预处理后的事件迭代器
      while (lineEntries.hasNext) {
        try {
          // 获取下一个事件
          val entry = lineEntries.next()

          // 记录当前事件并自增行号
          currentLine = entry._1
          lineNumber = entry._2 + 1

          // 投递事件
          postToAll(JsonProtocol.sparkEventFromJson(parse(currentLine)))
        } catch {
          case e: ClassNotFoundException if KNOWN_REMOVED_CLASSES.contains(e.getMessage) =>
            // Ignore events generated by Structured Streaming in Spark 2.0.0 and 2.0.1.
            // It's safe since no place uses them.
            logWarning(s"Dropped incompatible Structured Streaming log: $currentLine")
          case e: UnrecognizedPropertyException if e.getMessage != null && e.getMessage.startsWith(
            "Unrecognized field \"queryStatus\" " +
              "(class org.apache.spark.sql.streaming.StreamingQueryListener$") =>
            // Ignore events generated by Structured Streaming in Spark 2.0.2
            // It's safe since no place uses them.
            logWarning(s"Dropped incompatible Structured Streaming log: $currentLine")
          case jpe: JsonParseException =>
            // We can only ignore exception from last line of the file that might be truncated
            // the last entry may not be the very last line in the event log, but we treat it
            // as such in a best effort to replay the given input
            /**
              * 在解析JSON数据时出现解析错误，可能是因为文件被截断所导致的，
              * 此时需要根据maybeTruncated参数来决定是否抛出异常；
              * 如果允许截断文件存在，且没有更多的元素，则不抛出异常
              */
            if (!maybeTruncated || lineEntries.hasNext) {
              throw jpe
            } else {
              logWarning(s"Got JsonParseException from log file $sourceName" +
                s" at line $lineNumber, the file might not have finished writing cleanly.")
            }
        }
      }
    } catch {
      case ioe: IOException =>
        throw ioe
      case e: Exception =>
        logError(s"Exception parsing Spark event log: $sourceName", e)
        logError(s"Malformed line #$lineNumber: $currentLine\n")
    }
  }

}


private[spark] object ReplayListenerBus {

  type ReplayEventsFilter = (String) => Boolean

  // utility filter that selects all event logs during replay
  val SELECT_ALL_FILTER: ReplayEventsFilter = { (eventString: String) => true }

  /**
   * Classes that were removed. Structured Streaming doesn't use them any more. However, parsing
   * old json may fail and we can just ignore these failures.
   */
  val KNOWN_REMOVED_CLASSES = Set(
    "org.apache.spark.sql.streaming.StreamingQueryListener$QueryProgress",
    "org.apache.spark.sql.streaming.StreamingQueryListener$QueryTerminated"
  )
}
